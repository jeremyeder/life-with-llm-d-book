# Example Extraction Plan: llm-d-book-examples Repository

## Overview
Extract 43,182 words of code examples from the book to a separate repository, reducing book size by ~50% while improving maintainability and usability.

**Target Repository**: `github.com/jeremyeder/llm-d-book-examples`

---

## Repository Structure

```
llm-d-book-examples/
â”œâ”€â”€ README.md                    # Overview and navigation guide
â”œâ”€â”€ LICENSE                      # Matching book license
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ validate-yaml.yml    # YAML syntax validation
â”‚       â””â”€â”€ test-scripts.yml     # Script testing
â”œâ”€â”€ chapter-02-installation/
â”‚   â”œâ”€â”€ README.md               # Chapter context and examples list
â”‚   â”œâ”€â”€ basic-install.yaml      # Basic installation config
â”‚   â”œâ”€â”€ gpu-node-setup.yaml     # GPU node configuration
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ verify-install.sh   # Installation verification
â”œâ”€â”€ chapter-03-architecture/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ crd-examples/           # CRD examples
â”‚   â””â”€â”€ diagrams/               # Architecture diagrams
â”œâ”€â”€ chapter-04-data-scientist/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ model-deployment/       # Model deployment YAMLs
â”‚   â”œâ”€â”€ notebooks/              # Jupyter notebooks
â”‚   â””â”€â”€ scripts/                # Data science scripts
â”œâ”€â”€ chapter-05-sre-operations/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ monitoring/             # Prometheus/Grafana configs
â”‚   â”œâ”€â”€ alerts/                 # Alert rules
â”‚   â””â”€â”€ runbooks/              # Operational runbooks
â”œâ”€â”€ chapter-06-performance/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ benchmarks/            # Benchmark scripts
â”‚   â”œâ”€â”€ rdma-configs/          # RDMA configurations
â”‚   â””â”€â”€ optimization/          # Performance tuning configs
â”œâ”€â”€ chapter-07-security/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ rbac/                  # RBAC configurations
â”‚   â”œâ”€â”€ policies/              # Security policies
â”‚   â””â”€â”€ compliance/            # Compliance templates
â”œâ”€â”€ chapter-08-troubleshooting/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ diagnostic-scripts/    # Diagnostic tools
â”‚   â”œâ”€â”€ decision-trees/        # Troubleshooting flowcharts
â”‚   â””â”€â”€ common-fixes/          # Common issue resolutions
â”œâ”€â”€ chapter-10-mlops/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ pipelines/             # CI/CD pipeline definitions
â”‚   â”œâ”€â”€ tekton/                # Tekton pipelines
â”‚   â”œâ”€â”€ argo/                  # Argo workflows
â”‚   â””â”€â”€ testing/               # Test frameworks
â”œâ”€â”€ chapter-11-cost/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ calculators/           # Cost calculation scripts
â”‚   â””â”€â”€ optimization/          # Cost optimization configs
â””â”€â”€ reference/
    â”œâ”€â”€ commands.md            # Complete command reference
    â”œâ”€â”€ configurations/        # Template library
    â””â”€â”€ scripts/              # Utility scripts
```

---

## Extraction Rules

### Keep Inline (in book)
1. **Commands** (1-3 lines)
   ```bash
   kubectl get pods -n llm-d-system
   ```

2. **Key Configuration Snippets** (<10 lines)
   ```yaml
   spec:
     model: llama-3.1-8b
     replicas: 2
   ```

3. **Essential Error Messages**
   ```
   Error: GPU memory insufficient
   ```

4. **Quick Examples** that illustrate concepts

5. **Mermaid Diagrams** - Visual explanations that need context
   ```mermaid
   graph LR
     A[User] --> B[API Gateway]
     B --> C[Model Server]
   ```

6. **Key Benchmark Results** - Summary tables and performance graphs
   - Performance comparison charts
   - Critical benchmark images/graphs
   - Summary tables (tokens/sec, latency)
   - "Before/After" optimization results

7. **Inline Code** demonstrating key concepts

### Move to External Repo
1. **Complete YAML Files** (>10 lines)
2. **Full Scripts** (Python, Bash)
3. **Multi-file Examples**
4. **Extensive Configuration Templates**
5. **Raw Benchmark Data** and detailed logs
6. **Benchmark Scripts** and testing harnesses
7. **Complete Troubleshooting Procedures**
8. **Long Code Blocks** (>20 lines)
9. **Sample Data Files**
10. **Complete Working Examples**
11. **Detailed Performance Tuning Scripts**

---

## Migration Strategy

### Phase 1: Repository Setup (Day 1)
1. Create `llm-d-book-examples` repository
2. Set up directory structure
3. Create README with navigation
4. Set up CI/CD for validation

### Phase 2: Example Extraction (Days 2-5)
**Priority Order** (by impact):

1. **Chapter 10: MLOps** (15,874 words of code)
   - Extract complete pipeline definitions
   - Move Tekton/Argo workflow examples
   - Create working CI/CD templates

2. **Chapter 8: Troubleshooting** (11,292 words of code)
   - Extract diagnostic scripts
   - Move complete runbooks
   - Create executable troubleshooting tools

3. **Chapter 6: Performance** (8,350 words of code)
   - Extract benchmark scripts
   - Move RDMA configurations
   - Create performance testing suite

4. **Chapter 5: SRE Operations** (3,105 words of code)
   - Extract monitoring configs
   - Move alert definitions
   - Create operational templates

### Phase 3: Book Updates (Days 6-7)
1. Replace extracted code with references:
   ```markdown
   For the complete configuration, see:
   [Model Deployment Example](https://github.com/jeremyeder/llm-d-book-examples/tree/main/chapter-04-data-scientist/model-deployment/basic-deployment.yaml)
   ```

2. Add "Example Repository" section to each chapter
3. Update README with examples repo link

---

## Example Reference Format

### Before (in book):
```yaml
# 50+ lines of YAML configuration
apiVersion: llm.d.ai/v1alpha1
kind: Model
metadata:
  name: llama-3-1-8b
  namespace: production
spec:
  model: llama-3.1-8b
  # ... extensive configuration ...
```

### After (in book):
```markdown
Deploy the model using our production-ready configuration:

```yaml
# Basic structure
apiVersion: llm.d.ai/v1alpha1
kind: Model
metadata:
  name: llama-3-1-8b
spec:
  model: llama-3.1-8b
  replicas: 2
```

ðŸ“Ž **Full Example**: [production-deployment.yaml](https://github.com/jeremyeder/llm-d-book-examples/tree/main/chapter-04-data-scientist/model-deployment/production-deployment.yaml)

Key configuration points:
- GPU resource allocation
- Health check configuration
- Autoscaling parameters
```

### Link Formats to Use

1. **Inline Reference**:
   ```markdown
   For complete configuration, see the [production deployment example](https://github.com/jeremyeder/llm-d-book-examples/tree/main/chapter-04-data-scientist/model-deployment/production-deployment.yaml).
   ```

2. **Callout Box**:
   ```markdown
   > ðŸ“¦ **Example Files**: Find all monitoring configurations in the [examples repository](https://github.com/jeremyeder/llm-d-book-examples/tree/main/chapter-05-sre-operations/monitoring/)
   ```

3. **Chapter Footer**:
   ```markdown
   ## Example Code
   
   All examples from this chapter are available in the [llm-d-book-examples repository](https://github.com/jeremyeder/llm-d-book-examples/tree/main/chapter-05-sre-operations/):
   - [Monitoring Setup](./monitoring/)
   - [Alert Configurations](./alerts/)
   - [Operational Runbooks](./runbooks/)
   ```

4. **Try It Yourself Sections**:
   ```markdown
   ### Try It Yourself
   
   1. Clone the examples:
      ```bash
      git clone https://github.com/jeremyeder/llm-d-book-examples
      cd llm-d-book-examples/chapter-10-mlops/pipelines
      ```
   
   2. Deploy the CI/CD pipeline:
      ```bash
      kubectl apply -f tekton-pipeline.yaml
      ```
   ```

---

## Benefits Analysis

### Immediate Benefits
1. **Book Size Reduction**: ~43,000 words (55% reduction)
2. **Improved Readability**: Focus on concepts, not implementation
3. **Live Examples**: Users can clone and run immediately
4. **Version Control**: Examples can evolve independently

### Long-term Benefits
1. **Community Contributions**: Users can submit example improvements
2. **Testing**: Examples can be automatically tested
3. **Multiple Versions**: Support different llm-d versions
4. **Real-world Examples**: Accumulate production patterns

---

## Success Metrics

### Quantitative
- [ ] Book reduced by 40,000+ words
- [ ] All examples executable
- [ ] CI/CD validates all YAML
- [ ] Scripts have tests

### Qualitative
- [ ] Improved book flow and readability
- [ ] Examples are self-contained and runnable
- [ ] Clear navigation between book and examples
- [ ] Community engagement with examples repo

---

## Implementation Checklist

### Week 1
- [ ] Create llm-d-book-examples repository
- [ ] Set up directory structure
- [ ] Extract Chapter 10 (MLOps) examples
- [ ] Extract Chapter 8 (Troubleshooting) examples
- [ ] Update book chapters with references

### Week 2
- [ ] Extract remaining chapter examples
- [ ] Set up CI/CD validation
- [ ] Create navigation documentation
- [ ] Test all example links
- [ ] Final book word count validation

---

## Example README for External Repo

```markdown
# llm-d Book Examples

Complete, runnable examples from "Life with llm-d" book.

## Quick Start

```bash
git clone https://github.com/jeremyeder/llm-d-book-examples
cd llm-d-book-examples/chapter-04-data-scientist/model-deployment
kubectl apply -f basic-deployment.yaml
```

## Navigation

| Chapter | Topic | Examples |
|---------|-------|----------|
| [Chapter 2](./chapter-02-installation) | Installation | Basic setup, GPU nodes |
| [Chapter 4](./chapter-04-data-scientist) | Data Science | Model deployment, notebooks |
| [Chapter 5](./chapter-05-sre-operations) | SRE Ops | Monitoring, alerts, runbooks |
| [Chapter 8](./chapter-08-troubleshooting) | Troubleshooting | Diagnostic scripts, fixes |
| [Chapter 10](./chapter-10-mlops) | MLOps | CI/CD pipelines, automation |

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](./CONTRIBUTING.md).
```

---

**Next Step**: Create the external repository and begin extraction with highest-impact chapters (10, 8, 6).