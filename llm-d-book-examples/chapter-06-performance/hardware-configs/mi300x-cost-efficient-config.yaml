# AMD MI300X Cost-Efficient Configuration for Llama 3.1 70B
#
# AMD MI300X advantages leveraged:
# - 192GB HBM3e memory (2.4x H100 capacity)
# - Single GPU can handle 70B models (no tensor parallelism needed)
# - Cost-effective alternative to NVIDIA solutions
# - ROCm 7.0 optimizations for FP8 precision
# - Higher memory utilization possible (92%)
#
# Expected Performance:
# - Prefill: ~32 tokens/sec per replica
# - Decode: ~120 tokens/sec per replica  
# - Memory efficiency: Superior to H100 for large models
# - Total Cost: ~$19.20/hour for 4x MI300X setup (vs $18/hour H100)
# - Cost per token: ~20% lower than H100 due to single-GPU efficiency
#
# Source: Chapter 6 - Performance Optimization

apiVersion: serving.llm-d.ai/v1alpha1
kind: InferenceService
metadata:
  name: llama-3.1-70b-mi300x-efficient
  namespace: amd-production
spec:
  model:
    modelUri: "hf://meta-llama/Llama-3.1-70B-Instruct"
    quantization: "fp8"  # ROCm 7.0 FP8 support
    tensorParallelSize: 1  # Single MI300X can handle 70B
    maxModelLen: 16384
    
  serving:
    prefill:
      replicas: 4  # Multiple single-GPU instances
      resources:
        limits:
          amd.com/gpu: 1  # Single MI300X per replica
          memory: "256Gi"
        requests:
          amd.com/gpu: 1
          memory: "256Gi"
      
      # ROCm optimizations
      env:
      - name: ROC_GPU_MEMORY_UTILIZATION
        value: "0.92"  # MI300X memory efficiency
      - name: HIP_FORCE_DEV_KERNARG
        value: "1"  # ROCm performance optimization
      - name: ROCM_VERSION
        value: "7.0"
      - name: HSA_FORCE_FINE_GRAIN_PCIE
        value: "1"
      - name: VLLM_BACKEND
        value: "rocm"
        
    decode:
      replicas: 8
      resources:
        limits:
          amd.com/gpu: 1
          memory: "128Gi"
        requests:
          amd.com/gpu: 1  
          memory: "128Gi"
          
      # Decode-specific ROCm tuning
      env:
      - name: VLLM_DECODE_BATCH_SIZE
        value: "64"
      - name: HIP_VISIBLE_DEVICES
        value: "0"
        
  # AMD-specific node selection
  nodeSelector:
    amd.com/gpu.product: "MI300X"
    node.llm-d.ai/rocm-version: "7.0"
    
  # Cost optimization annotations
  annotations:
    cost.llm-d.ai/optimization-priority: "memory-efficiency"
    cost.llm-d.ai/single-gpu-preference: "true"