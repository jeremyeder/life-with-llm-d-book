---
title: v1.0.x Release Notes
description: llm-d v1.0.x Foundation Release Series - May-July 2025
---

# llm-d v1.0.x - Foundation Release Series

**Release Timeframe**: May - July 2025  
**Type**: Initial Release Series  
**Latest Version**: v1.0.22 (July 8, 2025)

## Overview

The v1.0.x series establishes the foundation for Kubernetes-native distributed inference serving. This series introduces core components and establishes the open source community infrastructure.

## üöÄ Core Features Introduced

### Platform Components

**llm-d-deployer**

- Single Helm chart deployment for complete platform setup
- Minikube and Kubernetes deployment support
- Automated dependency management and configuration
- Quickstart installation scripts

**llm-d-inference-scheduler**

- vLLM-optimized request routing and load balancing
- Gateway API integration with Envoy ext-proc support
- Pluggable scheduler architecture for custom routing logic
- Support for disaggregated prefill/decode workloads

**llm-d-kv-cache-manager**

- High-performance KV cache indexing and tracking
- Real-time metadata collection from vLLM instances
- Cache-aware routing for improved hit rates
- Cross-node cache coordination

**llm-d-benchmark**

- Automated inference performance testing framework
- Comprehensive metrics collection (latency, throughput, correctness)
- Support for multiple workload scenarios and model types
- Reproducible experiment configuration and results

### Development Tools

**llm-d-inference-sim**

- Lightweight vLLM simulator for development testing
- OpenAI-compatible API endpoint emulation
- Configurable response generation and timing
- Prometheus metrics integration

**llm-d-routing-sidecar**

- Experimental reverse proxy for P/D disaggregation
- Multiple connector support (nixl, nixlv2, lmcache)
- Side-channel communication for KV transfer
- Multi-GPU server support

## üìä Version History

### v1.0.22 (July 8, 2025)

- Latest stable release with bug fixes and improvements

### v1.0.21 (July 7, 2025)

- Performance optimizations and stability improvements

### v1.0.20 (June 23, 2025)

- Enhanced deployment reliability

### v1.0.18 (June 11, 2025)

- Configuration management improvements

### v1.0.9 (May 18, 2025)

- Initial open source release

## üîß Technical Specifications

### Hardware Support

- **NVIDIA GPUs**: H100, A100 series with CUDA optimization
- **AMD GPUs**: MI300X series with ROCm support
- **CPU Fallback**: x86_64 and ARM64 architecture support
- **Memory**: Dynamic allocation based on model requirements

### Kubernetes Integration

- **API Version**: Compatible with Kubernetes 1.24+
- **Custom Resources**: ModelService, InferenceScheduler CRDs
- **Networking**: Gateway API and Ingress support
- **Storage**: PVC and OCI image-based model storage
- **Monitoring**: Prometheus metrics and alerting

## ü§ù Community Establishment

### Founding Partners

- **CoreWeave**: Cloud infrastructure and GPU optimization
- **Google**: Kubernetes integration and cloud platform support  
- **IBM Research**: Advanced algorithms and research collaboration
- **NVIDIA**: GPU optimization and hardware acceleration
- **Red Hat**: Enterprise Kubernetes and OpenShift integration

### Supporting Partners

- **AMD**: Hardware acceleration and ROCm optimization
- **Cisco**: Networking and infrastructure solutions
- **Hugging Face**: Model hub integration and AI ecosystem
- **Intel**: CPU optimization and hardware acceleration
- **Lambda**: Cloud GPU infrastructure and optimization
- **Mistral AI**: Model development and optimization

## üîÑ Installation

### Requirements

- **Kubernetes**: Version 1.24 or later
- **Tools**: kubectl, Helm 3.x, yq, jq, git
- **Access**: Hugging Face token for model downloads
- **Resources**: Minimum 2 CPUs, 8GB RAM, GPU recommended

### Quickstart

```bash
# Clone the deployer
git clone https://github.com/llm-d/llm-d-deployer.git
cd llm-d-deployer/quickstart

# Set Hugging Face token
export HF_TOKEN="your-token"

# Run installer
./llmd-installer.sh
```

## üö® Known Issues

### Current Limitations

- Single model deployment per ModelService instance
- Limited LoRA adapter support
- Basic multi-tenancy and resource isolation
- Experimental disaggregated serving features

### Planned Improvements

See [What's Next](../00-whats-next.md) for v2.0.0 roadmap.

## üìû Support

### Community Resources

- **GitHub**: <https://github.com/llm-d>
- **Slack**: llm-d community workspace
- **Weekly Standups**: Contributor coordination meetings
- **Documentation**: Comprehensive guides and API reference

---

**Release Manager**: llm-d Community  
**License**: Apache 2.0  
**Next Major Release**: v2.0.0 expected July 17, 2025
